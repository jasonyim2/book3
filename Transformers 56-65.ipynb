{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [
        {
          "file_id": "1hcJkcxDH0A2yNCJ0EuitC1ycrvffOwUQ",
          "timestamp": 1667701126583
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y-JAGSXftu4"
      },
      "source": [
        "# 56"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMhAU3ojepVE",
        "executionInfo": {
          "elapsed": 15800,
          "status": "ok",
          "timestamp": 1688451068129,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          },
          "user_tz": -540
        },
        "outputId": "8acd7913-6c22-4a32-d2cd-136c82383b6b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFL77xxkgG3I"
      },
      "source": [
        "# 57"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwNbxukmfuR7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688451230515,
          "user_tz": -540,
          "elapsed": 1018,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "dataset = [[\"What music do you like?\", \"I like Rock music.\", 1],\n",
        "           [\"What is your favorite food?\", \"I like sushi the best\", 1],\n",
        "           [\"What is your favorite color?\", \"I'm going to be a doctor\", 0],\n",
        "           [\"What is your favorite song?\", \"Tokyo olympic game in 2020 was postponed\", 0],\n",
        "           [\"Do you like watching TV shows?\", \"Yeah, I often watch it in my spare time\", 1]]\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq4NxlhegU7I"
      },
      "source": [
        "# 58"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41kLJpTgHpl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688451241321,
          "user_tz": -540,
          "elapsed": 6988,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
        "from torch import nn\n",
        "\n",
        "# 클래스 정의\n",
        "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
        "  def __init__(self, config, *args, **kwargs):\n",
        "      super().__init__(config)\n",
        "\n",
        "      # QA BERT 모델\n",
        "      self.bert_model_1 = BertModel(config)\n",
        "      # AQ BERT 모델\n",
        "      self.bert_model_2 = BertModel(config)\n",
        "\n",
        "      # 선형함수\n",
        "      self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
        "      # 초기 가중치\n",
        "      self.init_weights()\n",
        "\n",
        "  def forward(\n",
        "          self,\n",
        "          input_ids=None,\n",
        "          attention_mask=None,\n",
        "          token_type_ids=None,\n",
        "          position_ids=None,\n",
        "          head_mask=None,\n",
        "          inputs_embeds=None,\n",
        "          next_sentence_label=None,\n",
        "  ):\n",
        "    # 빈 컨테이너 변수 outputs 생성\n",
        "    outputs = []\n",
        "\n",
        "    # input_ids 첫번째 입력(문장) 저장\n",
        "    input_ids_1 = input_ids[0]\n",
        "\n",
        "    # input_ids 첫번째 입력(문장)의 attention_mask 저장\n",
        "    attention_mask_1 = attention_mask[0]\n",
        "\n",
        "    # bert_model_1에 input_ids_1 투입한 결과를 outputs에 순차적으로 저장\n",
        "    outputs.append(self.bert_model_1(input_ids_1,\n",
        "                                     attention_mask=attention_mask_1))\n",
        "\n",
        "    # input_ids 두번째 입력(문장) 저장\n",
        "    input_ids_2 = input_ids[1]\n",
        "\n",
        "    # input_ids 두번째 입력(문장)의 attention_mask 저장\n",
        "    attention_mask_2 = attention_mask[1]\n",
        "\n",
        "    # bert_model_2에 input_ids_2 투입한 결과를 outputs에 순차적으로 저장\n",
        "    outputs.append(self.bert_model_2(input_ids_2,\n",
        "                                     attention_mask=attention_mask_2))\n",
        "\n",
        "    # outputs에 쌓인 output의 두번째 요소(output[1])을 하나하나 끄집어내어\n",
        "    # torch.cat()로 토치 텐서 형태로 병합\n",
        "    # 이를 통해 마지막 은닉층 임베딩 상태를 구함\n",
        "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
        "    # self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출\n",
        "    logits = self.cls(last_hidden_states)\n",
        "\n",
        "    # 크로스엔트로피 손실(crossentropyloss) 구하기\n",
        "    # crossentropyloss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "    if next_sentence_label is not None:\n",
        "      # nn.CrossEntropyLoss( ) 입력 데이터의 마지막 인덱스는 계산에서 제외\n",
        "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "      # logits.view(-1,2)는 열이 두 개 형태로 logits를 정렬\n",
        "      # next_sentence_label.view(-1)는 행이 하나인 형태(flattening)로 정렬\n",
        "      next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
        "      return next_sentence_loss, logits\n",
        "    else:\n",
        "      return logits\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ4peL0kgkSw"
      },
      "source": [
        "#59"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "ae7ccab201d741218f57ffc901cb6b4e",
            "9fd26a2a0ff04dc195fb238002ba6845",
            "c0d86ac8b72c4cfa96b01823a75304bb",
            "20d4462791a840358af6d6908784b526",
            "34c38df929284014b56c38f4dc154504",
            "0e521a5d1437478b8d0434a35e877a77",
            "ae6c9a5095064d9aab0a249c06385d7d",
            "5270feda257b48c2a1319a64c627f627",
            "595c177c9f5743aa9759f611fd94cf56",
            "38b3fc705dd349cb886d934cde7ecbb5",
            "942bebc31a08499fb1a8883a4875a5e9",
            "c90e5c6db5584656904aa998ee85cf1f",
            "fc5c8fd8ccd94ff1b895b6d05e3a2223",
            "5b2a6aae0afc49c6a7802f3567420c58",
            "ffde626bea074af78942df1b6247e9c8",
            "63f0a9ad1d3e481eaabfa83f41c98a1c",
            "81c1a892bc0048c6bfe610d1a8f84843",
            "4fde5e3c86e842ef98aab4097ae92be1",
            "9610c3f01c454c1682b3bb67a3b70cd0",
            "a518356809c24c95b29fdd6f9528a0b3",
            "0ee7e1904bf14ddbb444bed4f1e15d73",
            "240c534fd7194c4cad3c0ebb554f565c",
            "c28cfcb03db94537a2b883797081ba9c",
            "1ae0cf32f5cb4a8aa09dafa1434ca34f",
            "db912b8ad44a4dd7a442fb7e44dd8e46",
            "e345bb966b924dc0be2dc2bfe2c343a2",
            "5ce0062648b0468a8aeda511df4371bd",
            "51c9e7b1b41648c49f3211f3c0963faf",
            "3b97a92f8ca74c26b935dc59bba4b4e1",
            "38c7b0e9dd154916985995f7448f2f8b",
            "a1281864b5c6413fbd54233b5a382549",
            "4e650d8086174f30ba82d38d977610a5",
            "f9ecf218950a458b92f0c253ec32fbd7"
          ]
        },
        "id": "1QB0kgmOgV1Y",
        "executionInfo": {
          "elapsed": 13571,
          "status": "ok",
          "timestamp": 1688451258490,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          },
          "user_tz": -540
        },
        "outputId": "1b333377-a8c3-4cd0-ecbf-f535309bd643"
      },
      "source": [
        "import torch\n",
        "from transformers import AdamW\n",
        "\n",
        "# 코랩에서 가능한 경우 GPU를 사용하고, 그렇지 않으면 CPU 사용하도록\n",
        "# 변수 device로 설정을 저장\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델 및 config 설정\n",
        "config = BertConfig()\n",
        "model = BertEnsembleForNextSentencePrediction(config)\n",
        "\n",
        "# 모델을 변수 device에서 설정한 대로 GPU 혹은 CPU로 전송\n",
        "model.to(device)\n",
        "\n",
        "# 토크니이저 불러오기\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# 학습률 설정\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# 절편과 가중치를 no_decay 변수에 저장\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "# 최적화 함수 그룹 파라미터 설정\n",
        "optimizer_grouped_parameters = [{\n",
        "  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "  }]\n",
        "\n",
        "# 최적화 함수 설정\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae7ccab201d741218f57ffc901cb6b4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c90e5c6db5584656904aa998ee85cf1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c28cfcb03db94537a2b883797081ba9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe08dnQ0g2u3"
      },
      "source": [
        "# 60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdxpeaEglPE",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688451286467,
          "user_tz": -540,
          "elapsed": 499,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "#함수 prepare_data 정의\n",
        "def prepare_data(dataset, qa=True):\n",
        "\n",
        "  # 빈 컨테이너 변수 생성\n",
        "  input_ids, attention_masks = [], []\n",
        "  labels = []\n",
        "\n",
        "  for point in dataset:\n",
        "    if qa is True:\n",
        "      # point에 있는 3개의 원소를 앞에 요소부터 q, a, _ 으로 배정\n",
        "      q, a, _ = point\n",
        "    else:\n",
        "      # point에 있는 3개의 원소를 앞에 요소부터 a, q, _ 으로 배정\n",
        "      a, q, _ = point\n",
        "    # q와 a를 토크나이저를 통해 인코딩\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "      q,  # 문장 1 인코딩\n",
        "      a,  # 문장 2 인코딩\n",
        "      add_special_tokens=True,  # 특수 토큰인 [CLS]와 [SEP] 생성\n",
        "      max_length=128,           # max_length 설정Pad & truncate all sentences.\n",
        "      pad_to_max_length=True,   # max_length까지 패딩 실행\n",
        "      return_attention_mask=True,  # attention_mask 생성\n",
        "      return_tensors='pt',      # 파이토치 텐서 타입으로 출력\n",
        "      truncation=True           # truncation 실행\n",
        "    )\n",
        "\n",
        "    # encoded_dict(\"input_ids\")를 컨테이너 변수 input_ids에 순서대로 저장\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "    # encoded_dict(\"attention_mask\")를 컨테이너 변수 attention_mask에 순서대로 저장\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "    # point의 마지막(세번째) 요소(레이블)을 컨테이너 변수 labels에 순서대로 저장\n",
        "    labels.append(point[-1])\n",
        "    # 반복문 종료\n",
        "\n",
        "  # input_ids를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "\n",
        "  # attention_mask를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "  # 함수의 결과물로 input_ids, attention_mask, lables 출력\n",
        "  return input_ids, attention_masks, labels\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYs7hzphAls"
      },
      "source": [
        "# 61"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIRwyLPfg3Pr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688451290791,
          "user_tz": -540,
          "elapsed": 3,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
        "\n",
        "# QADataset 클래스 생성\n",
        "class QADataset(Dataset):\n",
        "\n",
        "  # input_ids 텐서와 attention_masks 텐서 생성\n",
        "  def __init__(self, input_ids, attention_masks, labels=None):\n",
        "    self.input_ids = np.array(input_ids)\n",
        "    self.attention_masks = np.array(attention_masks)\n",
        "    # toch.long은 정수(integer) 타입을 의미\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.attention_masks[index], self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_ids.shape[0]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csheuVwKhZ7J"
      },
      "source": [
        "# 62"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Blz1Xf3hBM2",
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1688451294336,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          },
          "user_tz": -540
        },
        "outputId": "21a27580-a343-4b25-af0d-9b903017ecd6"
      },
      "source": [
        "# dataset을 prepare_data 함수에 투입하여 그 결과를 각기\n",
        "# input_ids_qa, attention_maskes_qa, labels_qa에 저장\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "\n",
        "# 위의 세 결과물을 QADataset 클래스에 투입\n",
        "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "# 맨 윗줄 코드와 동일하나 이번에는 prepare_data 함수에 qa 플래그 값이 False일 때를 적용\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "\n",
        "# 바로 위 세 결과물을 QADataset 클래스에 투입\n",
        "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "# train_dataset_qa를 DataLoader로 처리\n",
        "dataloader_qa =  DataLoader(dataset=train_dataset_qa,\n",
        "                            batch_size=5,\n",
        "                            sampler=SequentialSampler(train_dataset_qa))\n",
        "# train_dataset_aq를 DataLoader로 처리\n",
        "dataloader_aq =  DataLoader(dataset=train_dataset_aq,\n",
        "                            batch_size=5,\n",
        "                            sampler=SequentialSampler(train_dataset_aq))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MplB819uh8sU"
      },
      "source": [
        "# 63"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB1ontGjhamH",
        "executionInfo": {
          "elapsed": 13185,
          "status": "ok",
          "timestamp": 1688451313366,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          },
          "user_tz": -540
        },
        "outputId": "b3e401b4-2409-419f-e861-8c5076bee812"
      },
      "source": [
        "# 런타임 20, 30초 이내, 단 GPU 미사용시 런타임 약 8분 소요\n",
        "# 에포크 횟수 지정\n",
        "epochs = 30\n",
        "\n",
        "# 에포크 횟수만큼 반복 루프 실행\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # dataloader_qa와 datalaode_aq 쌍(pair)를 동시에 반복 루프에서 처리\n",
        "  # enumerate 및 zip으로 두 데이터 쌍을 묶고 반복가능한 순서 부여\n",
        "  for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "    # enumerate로 묶은 두 데이터쌍을 순서대로 batch_1과 batch_2에 저장\n",
        "    batch_1, batch_2 = combined_batch\n",
        "    #모델을 학습 모드로 전환\n",
        "    model.train()\n",
        "\n",
        "    # 가능한 경우 batch_1과 batch_2의 데이터를 GPU로 전달하고\n",
        "    # 그렇지 않은 경우 CPU로 전달\n",
        "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "    # 모델에 투입할 변수 inputs의 내용 입력\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "    # 모델에 inputs를 **kwargs 형식(**inputs로 표기)으로 투입\n",
        "    # 즉, 딕셔너리 타입인 inputs의 키(key)와 키값(value) 모두 입력\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # 모델의 결과물인 outputs는 튜플 타입으로 출력\n",
        "    # 그중 첫번째 요소, 즉 outputs[0]을 변수 loss에 저장\n",
        "    loss = outputs[0]\n",
        "    # 오차 역전파\n",
        "    loss.backward()\n",
        "    # f 문자열을 사용하여 에포크와 손실 출력\n",
        "    print(f\"epoch:{epoch}, loss:{loss}\")\n",
        "\n",
        "    # 가중치 업데이트\n",
        "    optimizer.step()\n",
        "    # 다음 번 에포크를 위해 그래디언트(기울기) 초기화\n",
        "    model.zero_grad()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, loss:0.6825319528579712\n",
            "epoch:1, loss:0.7244614958763123\n",
            "epoch:2, loss:0.6235706210136414\n",
            "epoch:3, loss:0.5375152230262756\n",
            "epoch:4, loss:0.48787999153137207\n",
            "epoch:5, loss:0.5198245644569397\n",
            "epoch:6, loss:0.39239946007728577\n",
            "epoch:7, loss:0.386954128742218\n",
            "epoch:8, loss:0.3771936595439911\n",
            "epoch:9, loss:0.3023441433906555\n",
            "epoch:10, loss:0.2774304747581482\n",
            "epoch:11, loss:0.24556908011436462\n",
            "epoch:12, loss:0.2318638265132904\n",
            "epoch:13, loss:0.1650954633951187\n",
            "epoch:14, loss:0.15380802750587463\n",
            "epoch:15, loss:0.16936779022216797\n",
            "epoch:16, loss:0.10229847580194473\n",
            "epoch:17, loss:0.06644442677497864\n",
            "epoch:18, loss:0.045815031975507736\n",
            "epoch:19, loss:0.042585499584674835\n",
            "epoch:20, loss:0.03837328031659126\n",
            "epoch:21, loss:0.025047579780220985\n",
            "epoch:22, loss:0.02900034189224243\n",
            "epoch:23, loss:0.017099441960453987\n",
            "epoch:24, loss:0.017127936705946922\n",
            "epoch:25, loss:0.010944029316306114\n",
            "epoch:26, loss:0.006674528121948242\n",
            "epoch:27, loss:0.005496285390108824\n",
            "epoch:28, loss:0.005709633696824312\n",
            "epoch:29, loss:0.005998970475047827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d318KWuKRCIu"
      },
      "source": [
        "#64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh3Q-EbQh9a8",
        "executionInfo": {
          "elapsed": 494,
          "status": "ok",
          "timestamp": 1688451401592,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          },
          "user_tz": -540
        },
        "outputId": "82164f5a-eb31-4633-c5f1-07b084d45084"
      },
      "source": [
        "# dataset을 prepare_data 함수로 처리\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "# 위 결과물을 QADataset 클래스로 처리\n",
        "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "# dataset을 prepare_data 함수로 처리하되 qa 플래그를 False로 적용\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "# 위 결과물을 QADataset 클래스로 처리\n",
        "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "# test_dataset_qa를 DataLoader로 처리\n",
        "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
        "                            batch_size=16,\n",
        "                            sampler=SequentialSampler(test_dataset_qa))\n",
        "# test_dataset_aq를 DataLoader로 처리\n",
        "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
        "                            batch_size=16,\n",
        "                            sampler=SequentialSampler(test_dataset_aq))\n",
        "\n",
        "# 결과를 담을 빈 컨테이너로서 리스트 변수 생성\n",
        "complete_outputs, complete_label_ids = [], []\n",
        "\n",
        "# dataloader_qa와 dataloade_aq를 동시에 반복 루프 작업 실시\n",
        "\n",
        "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "\n",
        "  # 모델을 eval 모드로 전환\n",
        "  model.eval()\n",
        "  # enumerate로 묶은 두 데이터 쌍을 순서대로 batch_1과 batch_2에 저장\n",
        "  batch_1, batch_2 = combined_batch\n",
        "\n",
        "  # 가능한 경우 batch_1과 batch2의 데이터를 GPU로 전달하고\n",
        "  # 그렇지 않은 경우 CPU로 전달\n",
        "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "  # 추론(evaluation) 때는 순전파(forward propagation) 과정만 필요하기에\n",
        "  # 그래디언트(기울기) 계산이 필요 없음. 아래 코딩 줄은 자동미분을 방지.\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # 모델에 투입할 변수 inputs의 내용 입력\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "\n",
        "    # 모델에 inputs를 **kwargs 형식(**inputs로 표기)으로 투입\n",
        "    # 즉, 딕셔너리 타입인 inputs의 키(key)와 키값(value) 모두 입력\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # outputs의 첫번째 요소를 tmp_eval_loss에 저장\n",
        "    # outputs의 두번째 요소를 logits에 저장\n",
        "    tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "    # 로짓을 CPU로 전달하고 넘파이 값으로 변환\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    # logits에 담긴 로짓값을 축(axis) 1, 즉 가로방향으로 최대값을 갖는 인덱스 추출\n",
        "    outputs = np.argmax(logits, axis=1)\n",
        "    # inputs['next_sentence_label']을 CPU로 전달하고 넘파이 값으로 변환\n",
        "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
        "    # with torch.no_grad(): 구문 종료\n",
        "\n",
        "  # outputs과 label_ids를 각각의 컨테이너 리스트 변수에 순서대로 저장\n",
        "  complete_outputs.extend(outputs)\n",
        "  complete_label_ids.extend(label_ids)\n",
        "\n",
        "# 최종 결과물 출력\n",
        "print(complete_outputs, complete_label_ids)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 0, 1] [1, 1, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfFqrb5hjTX1"
      },
      "source": [
        "# 65"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1xq5Z_KirEA",
        "executionInfo": {
          "elapsed": 531,
          "status": "ok",
          "timestamp": 1688451409753,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          },
          "user_tz": -540
        },
        "outputId": "0fb9d475-2835-44a9-ee44-73d73fb2fc08"
      },
      "source": [
        "# 이 코드의 설명은 앞의 문제들 주석과 동일\n",
        "dataset = [[\"What music do you like?\", \"I like Rock music.\", 1]]\n",
        "\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
        "                            batch_size=16,\n",
        "                            sampler=SequentialSampler(test_dataset_qa))\n",
        "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
        "                            batch_size=16,\n",
        "                            sampler=SequentialSampler(test_dataset_aq))\n",
        "\n",
        "complete_outputs, complete_label_ids = [], []\n",
        "\n",
        "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "  model.eval()\n",
        "  batch_1, batch_2 = combined_batch\n",
        "\n",
        "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "    outputs = model(**inputs)\n",
        "    tmp_eval_loss, logits = outputs[:2]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    outputs = np.argmax(logits, axis=1)\n",
        "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
        "  complete_outputs.extend(outputs)\n",
        "  complete_label_ids.extend(label_ids)\n",
        "\n",
        "print(complete_outputs, complete_label_ids)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] [1]\n"
          ]
        }
      ]
    }
  ]
}