{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1hcJkcxDH0A2yNCJ0EuitC1ycrvffOwUQ","timestamp":1667701126583}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ae7ccab201d741218f57ffc901cb6b4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9fd26a2a0ff04dc195fb238002ba6845","IPY_MODEL_c0d86ac8b72c4cfa96b01823a75304bb","IPY_MODEL_20d4462791a840358af6d6908784b526"],"layout":"IPY_MODEL_34c38df929284014b56c38f4dc154504"}},"9fd26a2a0ff04dc195fb238002ba6845":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e521a5d1437478b8d0434a35e877a77","placeholder":"​","style":"IPY_MODEL_ae6c9a5095064d9aab0a249c06385d7d","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"c0d86ac8b72c4cfa96b01823a75304bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5270feda257b48c2a1319a64c627f627","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_595c177c9f5743aa9759f611fd94cf56","value":213450}},"20d4462791a840358af6d6908784b526":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38b3fc705dd349cb886d934cde7ecbb5","placeholder":"​","style":"IPY_MODEL_942bebc31a08499fb1a8883a4875a5e9","value":" 213k/213k [00:00&lt;00:00, 505kB/s]"}},"34c38df929284014b56c38f4dc154504":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e521a5d1437478b8d0434a35e877a77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae6c9a5095064d9aab0a249c06385d7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5270feda257b48c2a1319a64c627f627":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"595c177c9f5743aa9759f611fd94cf56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38b3fc705dd349cb886d934cde7ecbb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"942bebc31a08499fb1a8883a4875a5e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c90e5c6db5584656904aa998ee85cf1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc5c8fd8ccd94ff1b895b6d05e3a2223","IPY_MODEL_5b2a6aae0afc49c6a7802f3567420c58","IPY_MODEL_ffde626bea074af78942df1b6247e9c8"],"layout":"IPY_MODEL_63f0a9ad1d3e481eaabfa83f41c98a1c"}},"fc5c8fd8ccd94ff1b895b6d05e3a2223":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81c1a892bc0048c6bfe610d1a8f84843","placeholder":"​","style":"IPY_MODEL_4fde5e3c86e842ef98aab4097ae92be1","value":"Downloading (…)okenizer_config.json: 100%"}},"5b2a6aae0afc49c6a7802f3567420c58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9610c3f01c454c1682b3bb67a3b70cd0","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a518356809c24c95b29fdd6f9528a0b3","value":29}},"ffde626bea074af78942df1b6247e9c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ee7e1904bf14ddbb444bed4f1e15d73","placeholder":"​","style":"IPY_MODEL_240c534fd7194c4cad3c0ebb554f565c","value":" 29.0/29.0 [00:00&lt;00:00, 2.23kB/s]"}},"63f0a9ad1d3e481eaabfa83f41c98a1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81c1a892bc0048c6bfe610d1a8f84843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fde5e3c86e842ef98aab4097ae92be1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9610c3f01c454c1682b3bb67a3b70cd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a518356809c24c95b29fdd6f9528a0b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ee7e1904bf14ddbb444bed4f1e15d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"240c534fd7194c4cad3c0ebb554f565c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c28cfcb03db94537a2b883797081ba9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ae0cf32f5cb4a8aa09dafa1434ca34f","IPY_MODEL_db912b8ad44a4dd7a442fb7e44dd8e46","IPY_MODEL_e345bb966b924dc0be2dc2bfe2c343a2"],"layout":"IPY_MODEL_5ce0062648b0468a8aeda511df4371bd"}},"1ae0cf32f5cb4a8aa09dafa1434ca34f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51c9e7b1b41648c49f3211f3c0963faf","placeholder":"​","style":"IPY_MODEL_3b97a92f8ca74c26b935dc59bba4b4e1","value":"Downloading (…)lve/main/config.json: 100%"}},"db912b8ad44a4dd7a442fb7e44dd8e46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_38c7b0e9dd154916985995f7448f2f8b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1281864b5c6413fbd54233b5a382549","value":570}},"e345bb966b924dc0be2dc2bfe2c343a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e650d8086174f30ba82d38d977610a5","placeholder":"​","style":"IPY_MODEL_f9ecf218950a458b92f0c253ec32fbd7","value":" 570/570 [00:00&lt;00:00, 44.1kB/s]"}},"5ce0062648b0468a8aeda511df4371bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51c9e7b1b41648c49f3211f3c0963faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b97a92f8ca74c26b935dc59bba4b4e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38c7b0e9dd154916985995f7448f2f8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1281864b5c6413fbd54233b5a382549":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e650d8086174f30ba82d38d977610a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ecf218950a458b92f0c253ec32fbd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"6y-JAGSXftu4"},"source":["# 56"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMhAU3ojepVE","executionInfo":{"elapsed":15800,"status":"ok","timestamp":1688451068129,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"},"user_tz":-540},"outputId":"8acd7913-6c22-4a32-d2cd-136c82383b6b"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"RFL77xxkgG3I"},"source":["# 57"]},{"cell_type":"code","metadata":{"id":"hwNbxukmfuR7","executionInfo":{"status":"ok","timestamp":1688451230515,"user_tz":-540,"elapsed":1018,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"}}},"source":["dataset = [[\"What music do you like?\", \"I like Rock music.\", 1],\n","           [\"What is your favorite food?\", \"I like sushi the best\", 1],\n","           [\"What is your favorite color?\", \"I'm going to be a doctor\", 0],\n","           [\"What is your favorite song?\", \"Tokyo olympic game in 2020 was postponed\", 0],\n","           [\"Do you like watching TV shows?\", \"Yeah, I often watch it in my spare time\", 1]]\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aq4NxlhegU7I"},"source":["# 58"]},{"cell_type":"code","metadata":{"id":"O41kLJpTgHpl","executionInfo":{"status":"ok","timestamp":1688451241321,"user_tz":-540,"elapsed":6988,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"}}},"source":["from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n","from torch import nn\n","\n","# 클래스 정의\n","class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n","  def __init__(self, config, *args, **kwargs):\n","      super().__init__(config)\n","\n","      # QA BERT 모델\n","      self.bert_model_1 = BertModel(config)\n","      # AQ BERT 모델\n","      self.bert_model_2 = BertModel(config)\n","\n","      # 선형함수\n","      self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n","      # 초기 가중치\n","      self.init_weights()\n","\n","  def forward(\n","          self,\n","          input_ids=None,\n","          attention_mask=None,\n","          token_type_ids=None,\n","          position_ids=None,\n","          head_mask=None,\n","          inputs_embeds=None,\n","          next_sentence_label=None,\n","  ):\n","    # 빈 컨테이너 변수 outputs 생성\n","    outputs = []\n","\n","    # input_ids 첫번째 입력(문장) 저장\n","    input_ids_1 = input_ids[0]\n","\n","    # input_ids 첫번째 입력(문장)의 attention_mask 저장\n","    attention_mask_1 = attention_mask[0]\n","\n","    # bert_model_1에 input_ids_1 투입한 결과를 outputs에 순차적으로 저장\n","    outputs.append(self.bert_model_1(input_ids_1,\n","                                     attention_mask=attention_mask_1))\n","\n","    # input_ids 두번째 입력(문장) 저장\n","    input_ids_2 = input_ids[1]\n","\n","    # input_ids 두번째 입력(문장)의 attention_mask 저장\n","    attention_mask_2 = attention_mask[1]\n","\n","    # bert_model_2에 input_ids_2 투입한 결과를 outputs에 순차적으로 저장\n","    outputs.append(self.bert_model_2(input_ids_2,\n","                                     attention_mask=attention_mask_2))\n","\n","    # outputs에 쌓인 output의 두번째 요소(output[1])을 하나하나 끄집어내어\n","    # torch.cat()로 토치 텐서 형태로 병합\n","    # 이를 통해 마지막 은닉층 임베딩 상태를 구함\n","    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n","    # self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출\n","    logits = self.cls(last_hidden_states)\n","\n","    # 크로스엔트로피 손실(crossentropyloss) 구하기\n","    # crossentropyloss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n","    if next_sentence_label is not None:\n","      # nn.CrossEntropyLoss( ) 입력 데이터의 마지막 인덱스는 계산에서 제외\n","      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n","      # logits.view(-1,2)는 열이 두 개 형태로 logits를 정렬\n","      # next_sentence_label.view(-1)는 행이 하나인 형태(flattening)로 정렬\n","      next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n","      return next_sentence_loss, logits\n","    else:\n","      return logits\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQ4peL0kgkSw"},"source":["#59"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["ae7ccab201d741218f57ffc901cb6b4e","9fd26a2a0ff04dc195fb238002ba6845","c0d86ac8b72c4cfa96b01823a75304bb","20d4462791a840358af6d6908784b526","34c38df929284014b56c38f4dc154504","0e521a5d1437478b8d0434a35e877a77","ae6c9a5095064d9aab0a249c06385d7d","5270feda257b48c2a1319a64c627f627","595c177c9f5743aa9759f611fd94cf56","38b3fc705dd349cb886d934cde7ecbb5","942bebc31a08499fb1a8883a4875a5e9","c90e5c6db5584656904aa998ee85cf1f","fc5c8fd8ccd94ff1b895b6d05e3a2223","5b2a6aae0afc49c6a7802f3567420c58","ffde626bea074af78942df1b6247e9c8","63f0a9ad1d3e481eaabfa83f41c98a1c","81c1a892bc0048c6bfe610d1a8f84843","4fde5e3c86e842ef98aab4097ae92be1","9610c3f01c454c1682b3bb67a3b70cd0","a518356809c24c95b29fdd6f9528a0b3","0ee7e1904bf14ddbb444bed4f1e15d73","240c534fd7194c4cad3c0ebb554f565c","c28cfcb03db94537a2b883797081ba9c","1ae0cf32f5cb4a8aa09dafa1434ca34f","db912b8ad44a4dd7a442fb7e44dd8e46","e345bb966b924dc0be2dc2bfe2c343a2","5ce0062648b0468a8aeda511df4371bd","51c9e7b1b41648c49f3211f3c0963faf","3b97a92f8ca74c26b935dc59bba4b4e1","38c7b0e9dd154916985995f7448f2f8b","a1281864b5c6413fbd54233b5a382549","4e650d8086174f30ba82d38d977610a5","f9ecf218950a458b92f0c253ec32fbd7"]},"id":"1QB0kgmOgV1Y","executionInfo":{"elapsed":13571,"status":"ok","timestamp":1688451258490,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"},"user_tz":-540},"outputId":"1b333377-a8c3-4cd0-ecbf-f535309bd643"},"source":["import torch\n","from transformers import AdamW\n","\n","# 코랩에서 가능한 경우 GPU를 사용하고, 그렇지 않으면 CPU 사용하도록\n","# 변수 device로 설정을 저장\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 모델 및 config 설정\n","config = BertConfig()\n","model = BertEnsembleForNextSentencePrediction(config)\n","\n","# 모델을 변수 device에서 설정한 대로 GPU 혹은 CPU로 전송\n","model.to(device)\n","\n","# 토크니이저 불러오기\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","# 학습률 설정\n","learning_rate = 1e-5\n","\n","# 절편과 가중치를 no_decay 변수에 저장\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","# 최적화 함수 그룹 파라미터 설정\n","optimizer_grouped_parameters = [{\n","  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","  }]\n","\n","# 최적화 함수 설정\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7ccab201d741218f57ffc901cb6b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90e5c6db5584656904aa998ee85cf1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28cfcb03db94537a2b883797081ba9c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"qe08dnQ0g2u3"},"source":["# 60"]},{"cell_type":"code","metadata":{"id":"0cdxpeaEglPE","executionInfo":{"status":"ok","timestamp":1688451286467,"user_tz":-540,"elapsed":499,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"}}},"source":["#함수 prepare_data 정의\n","def prepare_data(dataset, qa=True):\n","\n","  # 빈 컨테이너 변수 생성\n","  input_ids, attention_masks = [], []\n","  labels = []\n","\n","  for point in dataset:\n","    if qa is True:\n","      # point에 있는 3개의 원소를 앞에 요소부터 q, a, _ 으로 배정\n","      q, a, _ = point\n","    else:\n","      # point에 있는 3개의 원소를 앞에 요소부터 a, q, _ 으로 배정\n","      a, q, _ = point\n","    # q와 a를 토크나이저를 통해 인코딩\n","    encoded_dict = tokenizer.encode_plus(\n","      q,  # 문장 1 인코딩\n","      a,  # 문장 2 인코딩\n","      add_special_tokens=True,  # 특수 토큰인 [CLS]와 [SEP] 생성\n","      max_length=128,           # max_length 설정Pad & truncate all sentences.\n","      pad_to_max_length=True,   # max_length까지 패딩 실행\n","      return_attention_mask=True,  # attention_mask 생성\n","      return_tensors='pt',      # 파이토치 텐서 타입으로 출력\n","      truncation=True           # truncation 실행\n","    )\n","\n","    # encoded_dict(\"input_ids\")를 컨테이너 변수 input_ids에 순서대로 저장\n","    input_ids.append(encoded_dict[\"input_ids\"])\n","    # encoded_dict(\"attention_mask\")를 컨테이너 변수 attention_mask에 순서대로 저장\n","    attention_masks.append(encoded_dict[\"attention_mask\"])\n","    # point의 마지막(세번째) 요소(레이블)을 컨테이너 변수 labels에 순서대로 저장\n","    labels.append(point[-1])\n","    # 반복문 종료\n","\n","  # input_ids를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n","  input_ids = torch.cat(input_ids, dim=0)\n","\n","  # attention_mask를 첫번째 축(dim=0), 즉 세로 방향으로 병합\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","\n","  # 함수의 결과물로 input_ids, attention_mask, lables 출력\n","  return input_ids, attention_masks, labels\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQYs7hzphAls"},"source":["# 61"]},{"cell_type":"code","metadata":{"id":"AIRwyLPfg3Pr","executionInfo":{"status":"ok","timestamp":1688451290791,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"}}},"source":["import numpy as np\n","from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n","\n","# QADataset 클래스 생성\n","class QADataset(Dataset):\n","\n","  # input_ids 텐서와 attention_masks 텐서 생성\n","  def __init__(self, input_ids, attention_masks, labels=None):\n","    self.input_ids = np.array(input_ids)\n","    self.attention_masks = np.array(attention_masks)\n","    # toch.long은 정수(integer) 타입을 의미\n","    self.labels = torch.tensor(labels, dtype=torch.long)\n","\n","  def __getitem__(self, index):\n","    return self.input_ids[index], self.attention_masks[index], self.labels[index]\n","\n","  def __len__(self):\n","    return self.input_ids.shape[0]\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"csheuVwKhZ7J"},"source":["# 62"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Blz1Xf3hBM2","executionInfo":{"elapsed":2,"status":"ok","timestamp":1688451294336,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"},"user_tz":-540},"outputId":"21a27580-a343-4b25-af0d-9b903017ecd6"},"source":["# dataset을 prepare_data 함수에 투입하여 그 결과를 각기\n","# input_ids_qa, attention_maskes_qa, labels_qa에 저장\n","input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n","\n","# 위의 세 결과물을 QADataset 클래스에 투입\n","train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n","\n","# 맨 윗줄 코드와 동일하나 이번에는 prepare_data 함수에 qa 플래그 값이 False일 때를 적용\n","input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n","\n","# 바로 위 세 결과물을 QADataset 클래스에 투입\n","train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n","\n","# train_dataset_qa를 DataLoader로 처리\n","dataloader_qa =  DataLoader(dataset=train_dataset_qa,\n","                            batch_size=5,\n","                            sampler=SequentialSampler(train_dataset_qa))\n","# train_dataset_aq를 DataLoader로 처리\n","dataloader_aq =  DataLoader(dataset=train_dataset_aq,\n","                            batch_size=5,\n","                            sampler=SequentialSampler(train_dataset_aq))\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"MplB819uh8sU"},"source":["# 63"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZB1ontGjhamH","executionInfo":{"elapsed":13185,"status":"ok","timestamp":1688451313366,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"},"user_tz":-540},"outputId":"b3e401b4-2409-419f-e861-8c5076bee812"},"source":["# 런타임 20, 30초 이내, 단 GPU 미사용시 런타임 약 8분 소요\n","# 에포크 횟수 지정\n","epochs = 30\n","\n","# 에포크 횟수만큼 반복 루프 실행\n","for epoch in range(epochs):\n","\n","  # dataloader_qa와 datalaode_aq 쌍(pair)를 동시에 반복 루프에서 처리\n","  # enumerate 및 zip으로 두 데이터 쌍을 묶고 반복가능한 순서 부여\n","  for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n","    # enumerate로 묶은 두 데이터쌍을 순서대로 batch_1과 batch_2에 저장\n","    batch_1, batch_2 = combined_batch\n","    #모델을 학습 모드로 전환\n","    model.train()\n","\n","    # 가능한 경우 batch_1과 batch_2의 데이터를 GPU로 전달하고\n","    # 그렇지 않은 경우 CPU로 전달\n","    batch_1 = tuple(t.to(device) for t in batch_1)\n","    batch_2 = tuple(t.to(device) for t in batch_2)\n","\n","    # 모델에 투입할 변수 inputs의 내용 입력\n","    inputs = {\n","        \"input_ids\": [batch_1[0], batch_2[0]],\n","        \"attention_mask\": [batch_1[1], batch_2[1]],\n","        \"next_sentence_label\": batch_1[2]\n","    }\n","    # 모델에 inputs를 **kwargs 형식(**inputs로 표기)으로 투입\n","    # 즉, 딕셔너리 타입인 inputs의 키(key)와 키값(value) 모두 입력\n","    outputs = model(**inputs)\n","\n","    # 모델의 결과물인 outputs는 튜플 타입으로 출력\n","    # 그중 첫번째 요소, 즉 outputs[0]을 변수 loss에 저장\n","    loss = outputs[0]\n","    # 오차 역전파\n","    loss.backward()\n","    # f 문자열을 사용하여 에포크와 손실 출력\n","    print(f\"epoch:{epoch}, loss:{loss}\")\n","\n","    # 가중치 업데이트\n","    optimizer.step()\n","    # 다음 번 에포크를 위해 그래디언트(기울기) 초기화\n","    model.zero_grad()\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:0, loss:0.6825319528579712\n","epoch:1, loss:0.7244614958763123\n","epoch:2, loss:0.6235706210136414\n","epoch:3, loss:0.5375152230262756\n","epoch:4, loss:0.48787999153137207\n","epoch:5, loss:0.5198245644569397\n","epoch:6, loss:0.39239946007728577\n","epoch:7, loss:0.386954128742218\n","epoch:8, loss:0.3771936595439911\n","epoch:9, loss:0.3023441433906555\n","epoch:10, loss:0.2774304747581482\n","epoch:11, loss:0.24556908011436462\n","epoch:12, loss:0.2318638265132904\n","epoch:13, loss:0.1650954633951187\n","epoch:14, loss:0.15380802750587463\n","epoch:15, loss:0.16936779022216797\n","epoch:16, loss:0.10229847580194473\n","epoch:17, loss:0.06644442677497864\n","epoch:18, loss:0.045815031975507736\n","epoch:19, loss:0.042585499584674835\n","epoch:20, loss:0.03837328031659126\n","epoch:21, loss:0.025047579780220985\n","epoch:22, loss:0.02900034189224243\n","epoch:23, loss:0.017099441960453987\n","epoch:24, loss:0.017127936705946922\n","epoch:25, loss:0.010944029316306114\n","epoch:26, loss:0.006674528121948242\n","epoch:27, loss:0.005496285390108824\n","epoch:28, loss:0.005709633696824312\n","epoch:29, loss:0.005998970475047827\n"]}]},{"cell_type":"markdown","metadata":{"id":"d318KWuKRCIu"},"source":["#64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sh3Q-EbQh9a8","executionInfo":{"elapsed":494,"status":"ok","timestamp":1688451401592,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"},"user_tz":-540},"outputId":"82164f5a-eb31-4633-c5f1-07b084d45084"},"source":["# dataset을 prepare_data 함수로 처리\n","input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n","# 위 결과물을 QADataset 클래스로 처리\n","test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n","\n","# dataset을 prepare_data 함수로 처리하되 qa 플래그를 False로 적용\n","input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n","# 위 결과물을 QADataset 클래스로 처리\n","test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n","\n","# test_dataset_qa를 DataLoader로 처리\n","dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n","                            batch_size=16,\n","                            sampler=SequentialSampler(test_dataset_qa))\n","# test_dataset_aq를 DataLoader로 처리\n","dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n","                            batch_size=16,\n","                            sampler=SequentialSampler(test_dataset_aq))\n","\n","# 결과를 담을 빈 컨테이너로서 리스트 변수 생성\n","complete_outputs, complete_label_ids = [], []\n","\n","# dataloader_qa와 dataloade_aq를 동시에 반복 루프 작업 실시\n","\n","for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n","\n","  # 모델을 eval 모드로 전환\n","  model.eval()\n","  # enumerate로 묶은 두 데이터 쌍을 순서대로 batch_1과 batch_2에 저장\n","  batch_1, batch_2 = combined_batch\n","\n","  # 가능한 경우 batch_1과 batch2의 데이터를 GPU로 전달하고\n","  # 그렇지 않은 경우 CPU로 전달\n","  batch_1 = tuple(t.to(device) for t in batch_1)\n","  batch_2 = tuple(t.to(device) for t in batch_2)\n","\n","  # 추론(evaluation) 때는 순전파(forward propagation) 과정만 필요하기에\n","  # 그래디언트(기울기) 계산이 필요 없음. 아래 코딩 줄은 자동미분을 방지.\n","  with torch.no_grad():\n","\n","    # 모델에 투입할 변수 inputs의 내용 입력\n","    inputs = {\n","        \"input_ids\": [batch_1[0], batch_2[0]],\n","        \"attention_mask\": [batch_1[1], batch_2[1]],\n","        \"next_sentence_label\": batch_1[2]\n","    }\n","\n","    # 모델에 inputs를 **kwargs 형식(**inputs로 표기)으로 투입\n","    # 즉, 딕셔너리 타입인 inputs의 키(key)와 키값(value) 모두 입력\n","    outputs = model(**inputs)\n","\n","    # outputs의 첫번째 요소를 tmp_eval_loss에 저장\n","    # outputs의 두번째 요소를 logits에 저장\n","    tmp_eval_loss, logits = outputs[:2]\n","\n","    # 로짓을 CPU로 전달하고 넘파이 값으로 변환\n","    logits = logits.detach().cpu().numpy()\n","    # logits에 담긴 로짓값을 축(axis) 1, 즉 가로방향으로 최대값을 갖는 인덱스 추출\n","    outputs = np.argmax(logits, axis=1)\n","    # inputs['next_sentence_label']을 CPU로 전달하고 넘파이 값으로 변환\n","    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n","    # with torch.no_grad(): 구문 종료\n","\n","  # outputs과 label_ids를 각각의 컨테이너 리스트 변수에 순서대로 저장\n","  complete_outputs.extend(outputs)\n","  complete_label_ids.extend(label_ids)\n","\n","# 최종 결과물 출력\n","print(complete_outputs, complete_label_ids)\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 0, 0, 1] [1, 1, 0, 0, 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"CfFqrb5hjTX1"},"source":["# 65"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1xq5Z_KirEA","executionInfo":{"elapsed":531,"status":"ok","timestamp":1688451409753,"user":{"displayName":"Jason Book Yim","userId":"02824935638305274154"},"user_tz":-540},"outputId":"0fb9d475-2835-44a9-ee44-73d73fb2fc08"},"source":["# 이 코드의 설명은 앞의 문제들 주석과 동일\n","dataset = [[\"What music do you like?\", \"I like Rock music.\", 1]]\n","\n","input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n","test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n","\n","input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n","test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n","\n","dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n","                            batch_size=16,\n","                            sampler=SequentialSampler(test_dataset_qa))\n","dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n","                            batch_size=16,\n","                            sampler=SequentialSampler(test_dataset_aq))\n","\n","complete_outputs, complete_label_ids = [], []\n","\n","for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n","  model.eval()\n","  batch_1, batch_2 = combined_batch\n","\n","  batch_1 = tuple(t.to(device) for t in batch_1)\n","  batch_2 = tuple(t.to(device) for t in batch_2)\n","\n","  with torch.no_grad():\n","    inputs = {\n","        \"input_ids\": [batch_1[0], batch_2[0]],\n","        \"attention_mask\": [batch_1[1], batch_2[1]],\n","        \"next_sentence_label\": batch_1[2]\n","    }\n","    outputs = model(**inputs)\n","    tmp_eval_loss, logits = outputs[:2]\n","    logits = logits.detach().cpu().numpy()\n","    outputs = np.argmax(logits, axis=1)\n","    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n","  complete_outputs.extend(outputs)\n","  complete_label_ids.extend(label_ids)\n","\n","print(complete_outputs, complete_label_ids)\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] [1]\n"]}]}]}