{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [
        {
          "file_id": "1hcJkcxDH0A2yNCJ0EuitC1ycrvffOwUQ",
          "timestamp": 1667701126583
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHrKkgKbQ5UD"
      },
      "source": [
        "#66"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5MnGG5GjSfH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c3e8e99bf59845818bb0bbff5345258b",
            "8e892cfb335b477e9ce6b6d1019cadcd",
            "8d8715f1fa6c4727958efe437ca57733",
            "c4943c0232dd4693814653b6e752e4fe",
            "f34a15a87e0845deaee1ce1903ab5240",
            "a2d72e677aac4788b04b42e3583d60c5",
            "03abc9e7c6544006906b2f443f6204e2",
            "aff557403dfe4e3993f2454b959b0881",
            "3221e9c124b54eaea218aaa69b0892d9",
            "ced44913c6e34ba5a856baeeb7ec867b",
            "d25679fde04e4a059483fa2ff2255be4",
            "bbd501292e6548b3b1b6c3abe08da5a8",
            "8de5d16c0e554fbda6b8bca9465df84e",
            "379bae423e16490f9aaed4f2ace06d7f",
            "ec2f50b0f33b409695f370f0a3b2dd64",
            "617228b6a2cf4c6c9bb214562881826b",
            "dc70d170ac814e61b43ead8c48779c2a",
            "6f823e6dc5d6409fb6c7177acd089bdb",
            "5031fa7aafa64285afdfe26a82b2c345",
            "d63aea5340944528ab08a5c578e7feec",
            "a5ab5cca6866494fa9b03090d34d971b",
            "0e61c89982c947db825d1e1346b340bf",
            "0cc0c028822344548b08f71ce8513981",
            "216acb87e7dd4b1a9a5ea805b4f39611",
            "c7df52c80ba1492290c231c3152273db",
            "b3299f8ad8f743a8a4c084a2daea5131",
            "fa11d9b1ef054d1795f8e4b5b0a786d9",
            "7386855b902a412199ebc624d503ba57",
            "dd754b7f1c06441e936a33231cd49175",
            "54f9e65daced4a6982211af08b092b8b",
            "9bcfb24c60434eae8ca2bfaed8818b03",
            "500e5b680ad24b568f772dfa760e4c7c",
            "e7636436824f408d8fa575cb141cd2df",
            "11c20e9163934d76b18496889d6043fc",
            "f38b8cbaf9fd47db9488305f631c1720",
            "5a75911a3da24a629aa9700b3d1bffc9",
            "41555cf4f7c047158cc3cf17f029c182",
            "9b83dbb0a52b408e89da347d55370079",
            "c040925a105444cb995740057b602668",
            "85492fb69500416ba3687a7e65116a90",
            "cd97722fd43940c080693d3d3bd5e99e",
            "9445736dde7046919f2e9adca3d3cf63",
            "e567e8c15c014c5992250946389fe8a9",
            "133da1ebec014711b02cdcd51bafb446",
            "25e24fa15b254f4ba4dc69b1919eeb75",
            "c8de4b2e5d32473a908df022d9d3545b",
            "276e29e415844243a194591af188c0cc",
            "0ea58c5843f94ec9ba62e3165adb96a1",
            "c5796b532b2c4772afb4385bfaeb8829",
            "aeb3a7912377423a8280b91f00f0996f",
            "152a05e815b243adb63d5086e03539d2",
            "0cd5843c284f4b1dbd21e187acf8fa4a",
            "2855e3a14d8d462a95941c1f58322876",
            "ed74deb7c3e54392b78403a41a1eadf4",
            "bce46275ee9d44579432c4a787f92ff7"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453017305,
          "user_tz": -540,
          "elapsed": 30769,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        },
        "outputId": "2bd21a8b-be18-4775-b7eb-e8fdb1853ea9"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "from transformers import BigBirdTokenizer, BigBirdForMaskedLM\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
        "model = BigBirdForMaskedLM.from_pretrained('google/bigbird-roberta-base')\n",
        "model\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/846k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3e8e99bf59845818bb0bbff5345258b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbd501292e6548b3b1b6c3abe08da5a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cc0c028822344548b08f71ce8513981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11c20e9163934d76b18496889d6043fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25e24fa15b254f4ba4dc69b1919eeb75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigBirdForMaskedLM(\n",
              "  (bert): BigBirdModel(\n",
              "    (embeddings): BigBirdEmbeddings(\n",
              "      (word_embeddings): Embedding(50358, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(4096, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BigBirdEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BigBirdLayer(\n",
              "          (attention): BigBirdAttention(\n",
              "            (self): BigBirdBlockSparseAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): BigBirdSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BigBirdIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): NewGELUActivation()\n",
              "          )\n",
              "          (output): BigBirdOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              "  (cls): BigBirdOnlyMLMHead(\n",
              "    (predictions): BigBirdLMPredictionHead(\n",
              "      (transform): BigBirdPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): NewGELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=50358, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIVliG4JT06J"
      },
      "source": [
        "#67"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Hj27BCThxn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453070746,
          "user_tz": -540,
          "elapsed": 474,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "inputs = [\"I like reading [MASK].\", \"I like driving a [MASK].\",\"The world is facing with a [MASK] [MASK] crisis. We are all suffering from infectious diseases.\"]\n",
        "answers = [\"I like reading book.\", \"I like driving a car.\", \"The world is facing with a pandemic crisis. We are all suffering from infectious diseases.\"]\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWQm4k97T220"
      },
      "source": [
        "#68"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2gva6SIT2XU",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453073265,
          "user_tz": -540,
          "elapsed": 261,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "# 리스트 타입의 빈 컨테이너 변수 준비\n",
        "encoded_inputs = []\n",
        "encoded_labels =  []\n",
        "\n",
        "# inputs와 answers를 zip( )을 통해 묶은 후\n",
        "# inputs와 answers의 입력 문장 하나씩 각기 i와 l 변수로 불러내어 반복 루프 실행\n",
        "for i, l in zip(inputs, answers):\n",
        "\n",
        "  # i와 l변수를 각기 토크나이저로 인코딩 후 컨테이너 변수에 각기 순서대로 저장\n",
        "  # 특히 아래 두번째 코드는 l변수의 토크나이저 인코딩 후 input_ids만 저장\n",
        "  encoded_inputs.append(tokenizer(i, return_tensors=\"pt\"))\n",
        "  encoded_labels.append(tokenizer(l, return_tensors=\"pt\")[\"input_ids\"])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baLm1cBEUYzS"
      },
      "source": [
        "# 69"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEfsQeKcUDPv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453079571,
          "user_tz": -540,
          "elapsed": 3218,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        },
        "outputId": "e7d63012-7087-4bc9-e543-885904f83b40"
      },
      "source": [
        "# encoded_inputs와 encoded_labels를 zip()을 통해 묶고\n",
        "# 그 둘의 요소를 각기 input과 label이라는 변수로 불러내어 반복 반복 루프 실행\n",
        "for input, label in zip(encoded_inputs, encoded_labels):\n",
        "\n",
        "  # 모델에 input과 label 투입\n",
        "  # input은 **kwargs 형태로서 키(key)와 키값(value) 모두 전달\n",
        "  outputs = model(**input, labels=label)\n",
        "  # 손실 추출\n",
        "  loss = outputs.loss\n",
        "  # 로짓 추출\n",
        "  logits = outputs.logits\n",
        "\n",
        "  # 손실 출력. 이때 item( )은 텐서 loss의 값을 파이썬 숫자로 추출\n",
        "  print(f\"loss：{loss.item()}\")\n",
        "\n",
        "  # len(logits[0])은 로짓의 첫번째 차원의 개수이며\n",
        "  # range(1, leg(logits[0]))은 1,2,3,...그리고 로짓의 첫번째 차원의 개수까지 범위\n",
        "  # logits[0][i]는 로짓의 첫번째 차원의 i번째 요소\n",
        "  # argmax(-1)은 logits[0][i]의 요소의 마지막 축(axis)에서 최대값을 갖는 인덱스를 추출\n",
        "  # 해당 인덱스를 기준으로 디코딩 후 그 결과를 순서대로 묶어서 출력\n",
        "  print(f\"prediction：{' '.join([tokenizer.decode(logits[0][i].argmax(-1)) for i in range(1, len(logits[0]))])}\")\n",
        "\n",
        "  # lable[0][1:-1]은 lable의 첫번째 차원의 처음과 끝을 제외한 모든 요소를 의미\n",
        "  # 이를 디코딩 후 출력\n",
        "  print(f\"answer：{tokenizer.decode(label[0][1:-1])}\")\n",
        "  # 출력시 줄바꾸기\n",
        "  print('\\n')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 7 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss：11.18355655670166\n",
            "prediction：i like reading it . i\n",
            "answer：I like reading book.\n",
            "\n",
            "\n",
            "loss：8.306910514831543\n",
            "prediction：its like driving a car . a\n",
            "answer：I like driving a car.\n",
            "\n",
            "\n",
            "loss：4.29605770111084\n",
            "prediction：the world is facing with a global health crisis . we are all suffering from infectious diseases . .\n",
            "answer：The world is facing with a pandemic crisis. We are all suffering from infectious diseases.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Deng7gB8V-0K"
      },
      "source": [
        "# 70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "f422dfc6f95c49709ec02c8f34fb412b",
            "a790307284a847deab1cd37dc6348400",
            "933bc413939845ddb12d4ab7e3da8ce2",
            "2bb6be9768464fb6a1d58876006f9568",
            "e1ee79a7b54a4abbbbeee9556eb6c187",
            "b9643612bcc94bb2aac8192556fd047f",
            "a0984623ed9247bebc3a2c5171edee1b",
            "21a742eb4225445c99f4743f41e8d7de",
            "cd4d827a6cca42eab6c83338b44f4eea",
            "0db13a7878484b2c96269b296754ee9c",
            "b5dc54ea7b1b417b94dd7add9bd8ef90",
            "2d49e7ef705a4e9e85a83e70c1d5f05d",
            "374a2608f9fb48418545cf7196a71078",
            "5567ae2be9b74d1f97176f29872b37d0",
            "0f4c181ac39c48e598a54b94fa6a3ef1",
            "bc5e99ff1aaf4ff4bc08feae147e0cbe",
            "8ae24c8a836541e0af2d0bf3cd89293e",
            "0327f67a9a044ca0a33ef6bca2ff0b52",
            "05d22c7e25b44d8da886c6da78816afb",
            "e69e396d9d064856ae374d7b0908312c",
            "7fa92d2fc09e4425a2c38031948350b7",
            "79277679fe874882a41f903f247209f6",
            "9c7f211e22374c839c3e3330bcc88674",
            "fadd45697a204478aac2ff49d31a2f5c",
            "6ddc43d6ea8e40778bbee6a10a41f0cb",
            "78ec5a19b93642a1835b16266efcec40",
            "e681252e9ee442af9afc427db08479ed",
            "d5274060372c4ddd9cd8beabcefad5e1",
            "bfbbd88f3718474da70c6e41c43780ef",
            "4b6539c83c774177b44a38920838a44d",
            "111675f2ae6245b78ac8ce13600fc3d1",
            "c8f071da48c542b49bada9a482a6c00a",
            "e7147373828e4900a01e58991770f79b",
            "ed8f55dbcf334a0a9ff07e4909e951a2",
            "792a94d5a83e441cab711743b84ded1f",
            "18dd4932e7b94b688b2b49b7cedcc46e",
            "098b157a71b64364bb4024bc36a1a455",
            "4d4bb64362bd40278d8adfb264f6e1fb",
            "61f336628eca4e39a3770ed1e8e6d312",
            "4edbc809566f4e84836d57f8d46d4860",
            "518b93e4e7c14b728367c341023f52cc",
            "9e809f5c5d0a4695b3672f51a365b61c",
            "7eb874273f1643768831fca5549c8698",
            "973e70115a2c42189bcb6e5c03e07694",
            "cfc412ad9c9b4cd29985a2f5422827c4",
            "1a50b13abd7b4018b2566b167c9c50a3",
            "731f57f061614c1da90b55cd399332b9",
            "60ffb8eef0a3473688303616c65fc525",
            "8bd20caa147041c39a5f8ee7534454fa",
            "3ef71a29151d4e058b766d3b51a2f2c9",
            "f04c40109cb14cc48207bdd605c57250",
            "f29f3d2e93a34910b766bd0516c99313",
            "5c08c1a1c1fd41339b4bc8613bb65358",
            "bcfc7222a76449009addb33d12249441",
            "e792afd0e51c425eb993502095d84f13",
            "f9d91c795ff9422799b98342fb9efed9",
            "9082e64bb11146bdaca7065ccfb9aeb7",
            "ed7aa784530b4ebf8030c4a846b11712",
            "5c65af598a7e484ba1e60aa29272f6a4",
            "2e7a679769304f5aba6b30460bfc7f01",
            "dede84823ab74d6695718d2325483f45",
            "efe5762346fc4983b8701f68ae7b4315",
            "c5369b43660e4f96a6b56b7511ce9c73",
            "37b24f9b4b564e8b95c2fc458abcb2f2",
            "202180d2689c41fa98e52fadcede5c74",
            "1837baad967b494dbec34cf00dbeac45"
          ]
        },
        "id": "Zbq42M82UZpR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453125643,
          "user_tz": -540,
          "elapsed": 41178,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        },
        "outputId": "a1828ef9-4b24-4929-a226-a430dff1bc66"
      },
      "source": [
        "# 런타임 1분 소요\n",
        "\n",
        "# 코랩이 메모리가 갱신(clear)된 경우만 아래 두 줄 코딩에서\n",
        "# 주석 기호(#)를 제거하고 실행\n",
        "#!pip install transformers\n",
        "#!pip install sentencepiece\n",
        "\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "# 모델명\n",
        "model_name = 'google/pegasus-xsum'\n",
        "\n",
        "# 가능한 경우 GPU를 사용하고, 그렇지 않은 경우 CPU를 사용\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 모델 및 토크나이저 불러오기\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f422dfc6f95c49709ec02c8f34fb412b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d49e7ef705a4e9e85a83e70c1d5f05d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c7f211e22374c839c3e3330bcc88674"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed8f55dbcf334a0a9ff07e4909e951a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfc412ad9c9b4cd29985a2f5422827c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9d91c795ff9422799b98342fb9efed9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai14ypOLWIZe"
      },
      "source": [
        "# 71"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMnZssbWCGH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453217720,
          "user_tz": -540,
          "elapsed": 281,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        }
      },
      "source": [
        "# 여러 문장을 한꺼번에 인용할 때는 인용부호로 \"\"\"를 사용\n",
        "inputs = [\n",
        "          \"\"\"\n",
        "          Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.\n",
        "          \"\"\"\n",
        "]\n",
        "\n",
        "# inputs에 담긴 문장을 토크나이저로 인코딩\n",
        "batch = tokenizer(inputs, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-r4VUrHWJqS"
      },
      "source": [
        "#72"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyISiYM2WKQs",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1688453224932,
          "user_tz": -540,
          "elapsed": 4180,
          "user": {
            "displayName": "Jason Book Yim",
            "userId": "02824935638305274154"
          }
        },
        "outputId": "ce66710a-3135-422b-8dfb-b27513a18d6a"
      },
      "source": [
        "# 입력문장의 인코딩 결과가 담긴 batch 변수를 **kwargs 형식으로 투입\n",
        "# model.generate()가 요약 문장을 생성하고 결과는 인코딩되어 있음\n",
        "translated = model.generate(**batch)\n",
        "\n",
        "# 위의 인코딩된 요약 문장을 디코딩\n",
        "# 디코딩 시 [CLS] 등의 특수 토큰은 무시\n",
        "generated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "\n",
        "# 디코딩 텐서의 첫번째 요소(?차원 추후 재확인 필요) 출력\n",
        "print(generated_text[0])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretraining large neural language models can lead to substantial gains over continual pretraining of general-domain language models.\n"
          ]
        }
      ]
    }
  ]
}